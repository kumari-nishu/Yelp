{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide & Deep Learning using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "import feature_engineering\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "    import tensorflow as tf\n",
    "    \n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data objects using defined functions\n",
    "\n",
    "# ratings, business, checkin, user, tips = feature_engineering.get_yelp_data()\n",
    "# user = feature_engineering.add_user_features(user, ratings, tips)\n",
    "# business = feature_engineering.add_item_features(business, checkin)\n",
    "# ratings = feature_engineering.add_features_to_ratings(ratings, user, business)\n",
    "# ratings_train, ratings_validation, ratings_test = feature_engineering.train_validation_test_split(years = 1)\n",
    "# ratings_recommend = feature_engineering.user_recommendation_options(ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data objects from saved pickle files\n",
    "\n",
    "user = pickle.load(open(\"data/user_feature_set.pkl\", \"rb\"))\n",
    "business = pickle.load(open(\"data/business_feature_set.pkl\", \"rb\"))\n",
    "\n",
    "ratings_train = pickle.load(open(\"data/ratings_train_5_years.pkl\", \"rb\"))\n",
    "ratings_validation = pickle.load(open(\"data/ratings_validation_5_years.pkl\", \"rb\"))\n",
    "ratings_test = pickle.load(open(\"data/ratings_test_5_years.pkl\", \"rb\"))\n",
    "ratings_recommend = pickle.load(open(\"data/ratings_recommendation_list.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide and deep column name format\n",
    "column_names = {\n",
    "    'Restaurants': 'restaurants', 'Food': 'food', 'Fast Food': 'fast_food', \n",
    "    'Nightlife': 'nightlife', 'American (Traditional)': 'american', 'Bars': 'bars', \n",
    "    'Mexican': 'mexican', 'Sandwiches': 'sandwiches', 'Pizza': 'pizza', 'Burgers': 'burgers'\n",
    "}\n",
    "\n",
    "continuous_columns = [\n",
    "    'review_count_x', 'review_count_y', 'average_stars', 'stars', 'fans_norm', 'friends_norm', 'elite_count',\n",
    "    'compliment_count', 'compliment_score', 'user_lifetime', 'total_hours',  'total_checkins', 'age_of_business'\n",
    "]\n",
    "    \n",
    "categorical_columns = [\n",
    "    'user_id', 'business_id', 'restaurants', 'food', 'fast_food', 'nightlife', \n",
    "    'american', 'bars', 'mexican', 'sandwiches', 'pizza', 'burgers'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix business category column format\n",
    "def format_columns(df, column_names):\n",
    "    df = df.rename(columns = column_names)\n",
    "    \n",
    "    for column in df.columns[17:27]:\n",
    "        df[column] = df[column].apply(lambda x: str(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Wide & Deep Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_and_deep_columns():\n",
    "    # build categorical columns\n",
    "    user_id = tf.contrib.layers.sparse_column_with_hash_bucket(\"user_id\", hash_bucket_size = 50000)\n",
    "    business_id = tf.contrib.layers.sparse_column_with_hash_bucket(\"business_id\", hash_bucket_size = 25000)\n",
    "\n",
    "    # categorical columns for presence of business category\n",
    "    restaurants = tf.contrib.layers.sparse_column_with_keys(column_name = \"restaurants\", keys = ['0', '1'])\n",
    "    food = tf.contrib.layers.sparse_column_with_keys(column_name = \"food\", keys = ['0', '1'])\n",
    "    fast_food = tf.contrib.layers.sparse_column_with_keys(column_name = \"fast_food\", keys = ['0', '1'])\n",
    "    nightlife = tf.contrib.layers.sparse_column_with_keys(column_name = \"nightlife\", keys = ['0', '1'])\n",
    "    american = tf.contrib.layers.sparse_column_with_keys(column_name = \"american\", keys = ['0', '1'])\n",
    "    bars = tf.contrib.layers.sparse_column_with_keys(column_name = \"bars\", keys = ['0', '1'])\n",
    "    mexican = tf.contrib.layers.sparse_column_with_keys(column_name = \"mexican\", keys = ['0', '1'])\n",
    "    sandwiches = tf.contrib.layers.sparse_column_with_keys(column_name = \"sandwiches\", keys = ['0', '1'])\n",
    "    pizza = tf.contrib.layers.sparse_column_with_keys(column_name = \"pizza\", keys = ['0', '1'])\n",
    "    burgers = tf.contrib.layers.sparse_column_with_keys(column_name = \"burgers\", keys = ['0', '1'])\n",
    "    \n",
    "    # build continuous columns\n",
    "    review_count_user = tf.contrib.layers.real_valued_column(\"review_count_x\")\n",
    "    review_count_business = tf.contrib.layers.real_valued_column(\"review_count_y\")\n",
    "    average_stars_user = tf.contrib.layers.real_valued_column(\"average_stars\")\n",
    "    average_stars_business = tf.contrib.layers.real_valued_column(\"stars\")\n",
    "    \n",
    "    fans = tf.contrib.layers.real_valued_column(\"fans_norm\")\n",
    "    friends = tf.contrib.layers.real_valued_column(\"friends_norm\")\n",
    "    \n",
    "    elite_count = tf.contrib.layers.real_valued_column(\"elite_count\")\n",
    "    user_lifetime = tf.contrib.layers.real_valued_column(\"user_lifetime\")\n",
    "    compliment_count = tf.contrib.layers.real_valued_column(\"compliment_count\")\n",
    "    compliment_score = tf.contrib.layers.real_valued_column(\"compliment_score\")\n",
    "    \n",
    "    total_hours = tf.contrib.layers.real_valued_column(\"total_hours\")\n",
    "    total_checkins = tf.contrib.layers.real_valued_column(\"total_checkins\")\n",
    "    age_of_business = tf.contrib.layers.real_valued_column(\"age_of_business\")\n",
    "    \n",
    "    rating_scale = [1, 2, 3, 4, 5]\n",
    "    avg_rating_user_bucket = tf.contrib.layers.bucketized_column(average_stars_user, boundaries=rating_scale)\n",
    "    avg_rating_business_bucket = tf.contrib.layers.bucketized_column(average_stars_business, boundaries=rating_scale)\n",
    "\n",
    "    # build wide columns\n",
    "    wide_columns = [\n",
    "        user_id, \n",
    "        business_id,\n",
    "        restaurants,\n",
    "        food,\n",
    "        fast_food,\n",
    "        nightlife,\n",
    "        american,\n",
    "        bars,\n",
    "        mexican,\n",
    "        sandwiches,\n",
    "        pizza,\n",
    "        burgers,\n",
    "        tf.contrib.layers.crossed_column([user_id, avg_rating_user_bucket], hash_bucket_size = int(1e4)),\n",
    "        tf.contrib.layers.crossed_column([business_id, avg_rating_business_bucket], hash_bucket_size = int(1e4))\n",
    "    ]\n",
    "    \n",
    "    # build deep columns\n",
    "    deep_columns = [\n",
    "        tf.contrib.layers.embedding_column(user_id, dimension = 100),\n",
    "        tf.contrib.layers.embedding_column(business_id, dimension = 100),\n",
    "        review_count_user, \n",
    "        review_count_business, \n",
    "        average_stars_user, \n",
    "        average_stars_business, \n",
    "        fans, \n",
    "        friends, \n",
    "        compliment_count,\n",
    "        compliment_score,\n",
    "        elite_count, \n",
    "        user_lifetime,\n",
    "        total_hours, \n",
    "        total_checkins, \n",
    "        age_of_business\n",
    "    ]\n",
    "    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "    for k in continuous_columns:\n",
    "        df[k] = pd.to_numeric(df[k])\n",
    "    \n",
    "    # creating tensors for continuous and categorical columns\n",
    "    continuous_cols = {k: tf.constant(df[k].values) for k in continuous_columns}\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices = [[i, 0] for i in range(df[k].size)],\n",
    "        values = df[k].values,\n",
    "        dense_shape = [df[k].size, 1]) for k in categorical_columns}\n",
    "    \n",
    "    # combining all feature columns\n",
    "    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    # specifying target variable\n",
    "    df['rating'] = df['rating'].apply(lambda x: int(x))\n",
    "    label = tf.constant(df['rating'].values)\n",
    "\n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(ratings_train)\n",
    "\n",
    "def validation_input_fn():\n",
    "    return input_fn(ratings_validation)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(ratings_test)\n",
    "\n",
    "def recommend_input_fn():\n",
    "    return input_fn(ratings_recommend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(ratings_train, dnn_units = [100, 50], restart = True):\n",
    "    model_dir = \"model/wide_and_deep\"\n",
    "    if restart == True:\n",
    "        if os.path.isdir('model/wide_and_deep'):\n",
    "            shutil.rmtree(os.path.abspath('model/wide_and_deep'))\n",
    "    \n",
    "    # specifying parameters for multi-class classifier\n",
    "    wide_deep_model = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "        model_dir = model_dir,\n",
    "        n_classes = 6,\n",
    "        linear_feature_columns = wide_and_deep_columns()[0],\n",
    "        dnn_feature_columns = wide_and_deep_columns()[1], dnn_hidden_units = dnn_units)\n",
    "    \n",
    "    # fitting model by reducing loss over 400 steps\n",
    "    wide_deep_model.fit(input_fn = train_input_fn, steps = 400)\n",
    "    \n",
    "    return wide_deep_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tune(ratings_train, ratings_validation):\n",
    "    # tuning hidden layers and nodes hyperparameter\n",
    "    dnn_units = [[100, 50], [500, 250, 50], [1000, 500, 100, 50]]\n",
    "\n",
    "    tune_results = pd.DataFrame(columns = ['model_config', 'loss', 'recall', 'precision', 'auc'])\n",
    "    for i in range(len(dnn_units)):\n",
    "        wide_deep_model = model_train(ratings_train, dnn_units[i])\n",
    "        results = wide_deep_model.evaluate(input_fn = validation_input_fn, steps = 1)\n",
    "        predictions = wide_deep_model.predict(input_fn = validation_input_fn, as_iterable = False)\n",
    "        recall, precision, auc = model_eval(predictions, ratings_validation)\n",
    "        loss = results[\"loss\"]\n",
    "        \n",
    "        tune_results = tune_results.append({'model_config': dnn_units[i], 'loss': loss, 'recall': recall, \n",
    "                                            'precision': precision, 'auc': auc}, ignore_index = True)\n",
    "        \n",
    "    return tune_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(wide_deep_model, ratings_test):\n",
    "    results = wide_deep_model.evaluate(input_fn = test_input_fn, steps = 1) \n",
    "    predictions = wide_deep_model.predict(input_fn = test_input_fn, as_iterable = False)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average = \"micro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    \n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    return roc_auc_score(y_test, y_pred, average = average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(predictions, ratings_test):\n",
    "    labels = np.unique(predictions)\n",
    "    recall = recall_score(ratings_test['rating'], predictions, average = 'micro', labels = labels)\n",
    "    precision = precision_score(ratings_test['rating'], predictions, average = 'micro', labels = labels)\n",
    "    auc = multiclass_roc_auc_score(ratings_test['rating'], predictions, average = 'micro')\n",
    "    \n",
    "    return recall, precision, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Generating Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_recommend(wide_deep_model, ratings_recommend, k):\n",
    "    # adding user and business features to each review\n",
    "    ratings_recommend = ratings_recommend.merge(business, on = 'business_id')\n",
    "    ratings_recommend = ratings_recommend.merge(user, on = 'user_id')\n",
    "    ratings_recommend['rating'] = 0.0\n",
    "    \n",
    "    column_names = {\n",
    "    'Restaurants': 'restaurants', 'Food': 'food', 'Fast Food': 'fast_food', \n",
    "    'Nightlife': 'nightlife', 'American (Traditional)': 'american', 'Bars': 'bars', \n",
    "    'Mexican': 'mexican', 'Sandwiches': 'sandwiches', 'Pizza': 'pizza', 'Burgers': 'burgers'\n",
    "    }\n",
    "    \n",
    "    ratings_recommend = ratings_recommend.rename(columns = column_names)\n",
    "    for column in ratings_recommend.columns[14:24]:\n",
    "        ratings_recommend[column] = ratings_recommend[column].apply(lambda x: str(x))\n",
    "    \n",
    "    # predict rating and return top-k businesses with highest rating\n",
    "    predictions = wide_deep_model.predict(input_fn = recommend_input_fn, as_iterable = False)\n",
    "    predictions = pd.DataFrame({'rating': predictions})\n",
    "    predictions = pd.concat([ratings_recommend[['user_id', 'business_id']], predictions], axis = 1)\n",
    "    predictions = predictions.groupby('user_id').apply(lambda x: x.nlargest(k, ['rating'])).reset_index(drop = True)\n",
    "    \n",
    "    predictions.to_pickle(\"data/wide_and_deep_recommendations.pkl\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Function Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting columns for wide and deep tensors\n",
    "ratings_train = format_columns(ratings_train, column_names)\n",
    "ratings_validation = format_columns(ratings_validation, column_names)\n",
    "ratings_test = format_columns(ratings_test, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a28210e10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'model/wide_and_deep'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 348.93814, step = 2\n",
      "INFO:tensorflow:global_step/sec: 0.727437\n",
      "INFO:tensorflow:loss = 1.4227837, step = 202 (226.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.15632\n",
      "INFO:tensorflow:global_step/sec: 2.79572\n",
      "INFO:tensorflow:loss = 1.4125013, step = 402 (72.996 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 402 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.4125013.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-16T14:11:36Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-16-14:11:47\n",
      "INFO:tensorflow:Saving dict for global step 402: accuracy = 0.44153264, global_step = 402, loss = 1.4101336\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a2af6c048>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'model/wide_and_deep'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 307.7867, step = 2\n",
      "INFO:tensorflow:global_step/sec: 0.487653\n",
      "INFO:tensorflow:loss = 1.411415, step = 202 (366.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.623005\n",
      "INFO:tensorflow:global_step/sec: 0.805566\n",
      "INFO:tensorflow:Saving checkpoints for 354 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.4006692, step = 402 (251.745 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 402 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.4006692.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-16T14:24:40Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-16-14:24:52\n",
      "INFO:tensorflow:Saving dict for global step 402: accuracy = 0.44659415, global_step = 402, loss = 1.4023668\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a7afb2d68>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'model/wide_and_deep'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 163.40997, step = 2\n",
      "INFO:tensorflow:global_step/sec: 0.253886\n",
      "INFO:tensorflow:Saving checkpoints for 156 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.4313895, step = 202 (732.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293604\n",
      "INFO:tensorflow:global_step/sec: 0.319034\n",
      "INFO:tensorflow:Saving checkpoints for 336 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.4128226, step = 402 (635.841 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 402 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.4128226.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-16T14:50:25Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-16-14:50:37\n",
      "INFO:tensorflow:Saving dict for global step 402: accuracy = 0.4413592, global_step = 402, loss = 1.4061816\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "           model_config      loss    recall  precision       auc\n",
      "0             [100, 50]  1.410134  0.491220   0.441533  0.650958\n",
      "1        [500, 250, 50]  1.402367  0.446594   0.446594  0.654121\n",
      "2  [1000, 500, 100, 50]  1.406182  0.557945   0.441359  0.650849\n",
      "\n",
      "Time Elapsed = 2803.2678 secs\n"
     ]
    }
   ],
   "source": [
    "# finding best hyperparameters for the model\n",
    "start = time.time()\n",
    "tune_results = model_tune(ratings_train, ratings_validation)\n",
    "end = time.time()\n",
    "print(tune_results)\n",
    "print('\\nTime Elapsed = '+str(np.round(end - start, 4))+' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_config</th>\n",
       "      <th>loss</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[100, 50]</td>\n",
       "      <td>1.410134</td>\n",
       "      <td>0.491220</td>\n",
       "      <td>0.441533</td>\n",
       "      <td>0.650958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[500, 250, 50]</td>\n",
       "      <td>1.402367</td>\n",
       "      <td>0.446594</td>\n",
       "      <td>0.446594</td>\n",
       "      <td>0.654121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1000, 500, 100, 50]</td>\n",
       "      <td>1.406182</td>\n",
       "      <td>0.557945</td>\n",
       "      <td>0.441359</td>\n",
       "      <td>0.650849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_config      loss    recall  precision       auc\n",
       "0             [100, 50]  1.410134  0.491220   0.441533  0.650958\n",
       "1        [500, 250, 50]  1.402367  0.446594   0.446594  0.654121\n",
       "2  [1000, 500, 100, 50]  1.406182  0.557945   0.441359  0.650849"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter tune results\n",
    "tune_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a4a5907b8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'model/wide_and_deep'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 102.64913, step = 2\n",
      "INFO:tensorflow:global_step/sec: 0.25081\n",
      "INFO:tensorflow:Saving checkpoints for 148 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.431942, step = 202 (777.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.264607\n",
      "INFO:tensorflow:global_step/sec: 0.298961\n",
      "INFO:tensorflow:Saving checkpoints for 314 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3980782, step = 402 (669.029 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 402 into model/wide_and_deep/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.3980782.\n",
      "DNNLinearCombinedClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x1a4a590f60>, 'linear_feature_columns': (_SparseColumnHashed(column_name='user_id', is_integerized=False, bucket_size=50000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='business_id', is_integerized=False, bucket_size=25000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='restaurants', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='food', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='fast_food', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='nightlife', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='american', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='bars', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='mexican', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='sandwiches', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='pizza', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='burgers', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('0', '1'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _CrossedColumn(columns=(_BucketizedColumn(source_column=_RealValuedColumn(column_name='average_stars', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(1, 2, 3, 4, 5)), _SparseColumnHashed(column_name='user_id', is_integerized=False, bucket_size=50000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_SparseColumnHashed(column_name='business_id', is_integerized=False, bucket_size=25000, lookup_config=None, combiner='sum', dtype=tf.string), _BucketizedColumn(source_column=_RealValuedColumn(column_name='stars', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(1, 2, 3, 4, 5))), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None)), 'linear_optimizer': None, 'joint_linear_weights': False, 'dnn_feature_columns': (_EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='user_id', is_integerized=False, bucket_size=50000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=100, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1a432e67f0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True), _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='business_id', is_integerized=False, bucket_size=25000, lookup_config=None, combiner='sum', dtype=tf.string), dimension=100, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1a4a590630>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True), _RealValuedColumn(column_name='review_count_x', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='review_count_y', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='average_stars', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='stars', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='fans_norm', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='friends_norm', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='compliment_count', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='compliment_score', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='elite_count', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='user_lifetime', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='total_hours', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='total_checkins', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='age_of_business', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)), 'dnn_optimizer': None, 'dnn_hidden_units': [1000, 500, 100, 50], 'dnn_activation_fn': <function relu at 0x1a3313fea0>, 'dnn_dropout': None, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_partitioner': None, 'fix_global_step_increment_bug': False})\n",
      "\n",
      "Time Elapsed = 1593.6241 secs\n"
     ]
    }
   ],
   "source": [
    "# choosing best hyperparameter based on loss, auc, precision, recall\n",
    "opt_dnn_units = [1000, 500, 100, 50]\n",
    "\n",
    "# training model with optimum dnn hidden layers as parameter\n",
    "start = time.time()\n",
    "wide_deep_model = model_train(ratings_train, dnn_units = opt_dnn_units)\n",
    "end = time.time()\n",
    "print(wide_deep_model)\n",
    "print('\\nTime Elapsed = '+str(np.round(end - start, 4))+' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-12-16T15:22:58Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-16-15:23:04\n",
      "INFO:tensorflow:Saving dict for global step 402: accuracy = 0.4713655, global_step = 402, loss = 1.402535\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Time Elapsed = 19.9261 secs\n"
     ]
    }
   ],
   "source": [
    "# predict rating on test set using best tuned model\n",
    "start = time.time()\n",
    "predictions = model_predict(wide_deep_model, ratings_test)\n",
    "end = time.time()\n",
    "print('\\nTime Elapsed = '+str(np.round(end - start, 4))+' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Test Data:\n",
      "\n",
      "AUC =  0.6696\n",
      "Recall =  0.6797\n",
      "Precision =  0.4714\n",
      "\n",
      "Time Elapsed = 0.0495 secs\n"
     ]
    }
   ],
   "source": [
    "# evaluating model predictions on test set - recall, precision, auc\n",
    "start = time.time()\n",
    "test_recall, test_precision, test_auc = model_eval(predictions, ratings_test)\n",
    "end = time.time()\n",
    "print('For Test Data:\\n')\n",
    "print('AUC = ', np.round(test_auc, 4))\n",
    "print('Recall = ', np.round(test_recall, 4))\n",
    "print('Precision = ', np.round(test_precision, 4))\n",
    "print('\\nTime Elapsed = '+str(np.round(end - start, 4))+' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model/wide_and_deep/model.ckpt-402\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Time Elapsed = 1278.0347 secs\n"
     ]
    }
   ],
   "source": [
    "# generate top-k business recommendations for each user\n",
    "start = time.time()\n",
    "recommendations = model_recommend(wide_deep_model, ratings_recommend, k = 10)\n",
    "end = time.time()\n",
    "print('\\nTime Elapsed = '+str(np.round(end - start, 4))+' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ccePVt9OVohny-iuSFhpUw</td>\n",
       "      <td>6xL0DuOl6PkkzyaJTCPtAw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0baVos9lkiA6iodVBpSw7w</td>\n",
       "      <td>qUSisWf5QNIMk8aeVhwKjA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aJ92dlKbQudiqGPctHti4w</td>\n",
       "      <td>u8-WDsLXAl0dXQW_wqWrDg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oOeVwHnFiIazPaXR_I7TDw</td>\n",
       "      <td>nWuomYtU748irUOWo_xohg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jkhkhta1T4Jy8erZggJa0w</td>\n",
       "      <td>qKys2eEhVt5A7NUYMn7o7Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>y3fSCjsHTLOh65E1hAaxDQ</td>\n",
       "      <td>06MlxbtB4ZYeg_ri02RIAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RdTdVgd9u58BA52d0eHqcQ</td>\n",
       "      <td>-0RkJ_uIduNLWQrphbADRw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PZht-dxaK6iFUwiLqHZqVw</td>\n",
       "      <td>2PS9kBbuJcmBhcNp-D62uA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MA-kMxn3PbY4uEn366x35Q</td>\n",
       "      <td>MAuSh7NKoT__CQ4BumoayA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59GSwMflQFllOAWvw0F5mw</td>\n",
       "      <td>AN6HjBZeEcLgekQawgmS3w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id\n",
       "0  ccePVt9OVohny-iuSFhpUw  6xL0DuOl6PkkzyaJTCPtAw\n",
       "1  0baVos9lkiA6iodVBpSw7w  qUSisWf5QNIMk8aeVhwKjA\n",
       "2  aJ92dlKbQudiqGPctHti4w  u8-WDsLXAl0dXQW_wqWrDg\n",
       "3  oOeVwHnFiIazPaXR_I7TDw  nWuomYtU748irUOWo_xohg\n",
       "4  jkhkhta1T4Jy8erZggJa0w  qKys2eEhVt5A7NUYMn7o7Q\n",
       "5  y3fSCjsHTLOh65E1hAaxDQ  06MlxbtB4ZYeg_ri02RIAQ\n",
       "6  RdTdVgd9u58BA52d0eHqcQ  -0RkJ_uIduNLWQrphbADRw\n",
       "7  PZht-dxaK6iFUwiLqHZqVw  2PS9kBbuJcmBhcNp-D62uA\n",
       "8  MA-kMxn3PbY4uEn366x35Q  MAuSh7NKoT__CQ4BumoayA\n",
       "9  59GSwMflQFllOAWvw0F5mw  AN6HjBZeEcLgekQawgmS3w"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 331.61478,
   "position": {
    "height": "353.837px",
    "left": "1018.19px",
    "right": "20px",
    "top": "39.9132px",
    "width": "566.372px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
